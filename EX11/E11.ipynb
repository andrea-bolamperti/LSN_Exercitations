{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/logo.jpg\" width=\"600\">\n",
    "\n",
    "## Exercitation_11 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I only report the plots, the results, and some comment. The code is in the Jupyter Notebook `NN`. In the directory I included the given Jupyter Notebooks `LSN_Python_Lecture_05` and the one of this exercitation, from which I took the starting code and lineguide.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 01\n",
    "\n",
    "In the Jupyter Notebook `NN` you can find the code I used to explore different combinations and values of $N_{\\mathrm{epochs}}, \\sigma,   N_{\\mathrm{train}} $ and the istructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to modify the parameters, I report the starting situation, which I use to compare \n",
    "($m$ is the slope and $b$ is the intersect of the linear slope).\n",
    "\n",
    "- $N_{\\mathrm{epochs}}=30$,   $\\sigma=0.$,   $N_{\\mathrm{train}} =100 $  \n",
    " $$ \\Rightarrow \\quad m=1.209 \\quad \\quad b = 0.880 \\quad \\quad \\text{Test accuracy}=0.2248      $$\n",
    " <img src=\"Images/fig1.jpg\" width=\"800\">\n",
    "\n",
    "\n",
    "Then I changed $N_{\\mathrm{epochs}}$:\n",
    "- $N_{\\mathrm{epochs}}=50$,   $\\sigma=0.$,   $N_{\\mathrm{train}} =100 $  \n",
    "$$ \\Rightarrow \\quad m=1.876 \\quad \\quad b = 0.973 \\quad \\quad \\text{Test accuracy}=0.0062      $$\n",
    " <img src=\"Images/fig2.jpg\" width=\"800\">\n",
    "- $N_{\\mathrm{epochs}}=100$,   $\\sigma=0.$,   $N_{\\mathrm{train}} =100 $  \n",
    "$$ \\Rightarrow \\quad m=1.900 \\quad \\quad b = 0.993 \\quad \\quad \\text{Test accuracy}=0.0031     $$\n",
    " <img src=\"Images/fig3.jpg\" width=\"800\">\n",
    " \n",
    "\n",
    "Varying $N_{\\mathrm{train}} $ I got:\n",
    "- $N_{\\mathrm{epochs}}=30$,   $\\sigma=0.$,   $N_{\\mathrm{train}} =300 $  \n",
    "$$ \\Rightarrow \\quad m=1.749 \\quad \\quad b = 1.003 \\quad \\quad \\text{Test accuracy}=0.0293     $$\n",
    " <img src=\"Images/fig4.jpg\" width=\"800\">\n",
    "- $N_{\\mathrm{epochs}}=30$,   $\\sigma=0.$,   $N_{\\mathrm{train}} =1000 $  \n",
    "$$ \\Rightarrow \\quad m=1.997 \\quad \\quad b = 0.999 \\quad \\quad \\text{Test accuracy}=3.217 \\times 10^{-3}     $$\n",
    " <img src=\"Images/fig5.jpg\" width=\"800\">\n",
    " \n",
    " \n",
    "Finally, I tried to set some $\\sigma >0 $ :  \n",
    "The validation data with increasing $\\sigma $ are \n",
    " <img src=\"Images/figsigma.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I expected, the noise reduces the precision of the data and the accuracy of the fit, but it can well balanced increasing $N_{\\mathrm{epochs}}$ and $N_{\\mathrm{train}}$, which increase the accuracy.  \n",
    "With $\\sigma>0$, I have to be careful in avoiding *overfitting*, i.e. not to fit also the white noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 02\n",
    "\n",
    "I started from the code of the previous part. I decided to use a neuron in input and a neuron in output, because the function has only a variable and real values.  \n",
    "\n",
    "At the beginning, I used a *sigmoid* activation function for the hidden layer(s) and, all along this part, I fixed the values  \n",
    "    $N_{\\mathrm{epochs}}=30$,   $\\sigma=0.05$,   $N_{\\mathrm{train}} =1000 $,   $N_{\\mathrm{validation}} =10^4 $.  \n",
    "    \n",
    "The parameters I changed are: \n",
    "\n",
    " - the number of neurons in each layer   \n",
    " - the number of  <a href=\"https://keras.io/layers/about-keras-layers/\">layers</a>\n",
    " - the <a href=\"https://keras.io/activations/\">activation functions</a>\n",
    " - the <a href=\"https://keras.io/optimizers/\">optimizer</a>\n",
    " - the <a href=\"https://keras.io/losses/\"> loss</a> function.  \n",
    "\n",
    "Other important and interesting informations are in the Jupyter Notebook `LSN_Python_Lecture_05`, which I included in this directory.\n",
    "    \n",
    "The code I used in this part is in the Jupyter Notebook `NN`, where I allow the user to vary the parameter comfortably in the first mirror. The only manipulation one has to do in the code is to add manually a new layer.  \n",
    "    \n",
    "In all the reported tries, I did not change the <a href=\"https://keras.io/metrics/\">metrics</a>, I only used the *mse* (**mean square error**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\figt1.jpg\" width=\"800\">   \n",
    "<img src=\"Images\\figt2.jpg\" width=\"800\">\n",
    "\n",
    "$$ \\Rightarrow \\quad  \\text{Test accuracy}=0.1968    $$\n",
    "    \n",
    "In the first try, I used 1 hidden layer of 15 neurons, and I got a result with a shape similar to the one desired, but still very different. I tried to add more neurons in the second try, to see how the fit would change.  \n",
    "I see there is no a big difference, so I decided to act on another parameter, i.e. I added more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\figt3.jpg\" width=\"800\"> \n",
    "$$ \\Rightarrow \\quad  \\text{Test accuracy}=0.1163    $$    \n",
    "<img src=\"Images\\figt4.jpg\" width=\"800\">\n",
    "$$ \\Rightarrow \\quad  \\text{Test accuracy}=0.2254    $$\n",
    "    \n",
    "The fit seems to be better in the third try, in particular in the region on the rigth, near the local minimum. I can't see an upgrade in the results adding a third hidden layer ($4^\\mathrm{th}$ try), so I decided to continue with only two of them.  \n",
    "The next step has been to modify the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\figt5.jpg\" width=\"800\"> \n",
    "$$ \\Rightarrow \\quad  \\text{Test accuracy}=0.0013    $$\n",
    "    \n",
    "With *selu* (scaled Exponential Linear Unit) activation function, the improvement is evident. The only region not fitted in a satisfactory way is the minimum on the right, I try to make it better with a different optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\figt6.jpg\" width=\"800\"> \n",
    "$$ \\Rightarrow \\quad  \\text{Test accuracy}=0.0012    $$\n",
    "    \n",
    "The tail is better fitted with this optimizer. Finally, I try to change the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\figt7.jpg\" width=\"800\"> \n",
    "\n",
    "$$ \\Rightarrow \\quad  \\text{Test accuracy}=0.00054    $$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data prediction outside of the training interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this last result, I tried to see how this NN predict points outside the training interval.  \n",
    "\n",
    "<img src=\"Images\\out1.jpg\" width=\"700\"> \n",
    "\n",
    "The fit is very good in the training interval [-1;1], but it quickly lose its goodness outside of this interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I extended the code of the previous part to fit the 2D function $f(x,y) = \\sin(x^2+y^2)$ in the range $x \\in [-3/2,3/2]$ and $y \\in [-3/2,3/2]$.  \n",
    "I use $10^4$ training data $(x,y)$ and $1000$ validation data.  \n",
    "\n",
    "I modify the first layer, because I have now 2 variables $x,y$ in input. The scheme of the NN now is:\n",
    "* I layer of input with 2 neurons;\n",
    "* II layer of 40 neurons and function of activation *sigmoid*;\n",
    "* III and IV layers of 50 and 20 neurons respectively and function of activation *elu*; \n",
    "* V layer of output with 1 neuron.\n",
    "\n",
    "I fixed $\\sigma=0.05 $ and $N_{\\mathrm{epochs}}=50$. The optimizer is *Adam* and the metric and loss function are *mse*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td><img src='Images/plot.2D_1.jpg'></td><td><img src='Images/plot.2D_2.jpg'></td></tr></table>\n",
    "\n",
    "$$ \\Rightarrow \\quad  \\text{Test accuracy}=0.0038    $$    \n",
    "\n",
    "I tried also to add more layers or to vary some parameters, i.e. the activation functions, the optimizer and the number of neurons.  \n",
    "Again, the model fits the data very well in the training interval, but fails outside of it, as I can see in the tails of the function in the figure above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
